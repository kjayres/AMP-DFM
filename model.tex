% AMP-DFM: Mathematical formulation and implementation notes

\section*{Model overview}
We consider discrete sequences over a vocabulary of size \(V=24\). Tokens include a special CLS token at index 0, EOS at index 2, and amino acids indexed in \([4,23]\). A sequence of length \(L\) is represented as \(x\in [V]^L\), where the CLS token occupies position 0 and EOS terminates the sequence.

\subsection*{Tokenisation}
Let \(\mathrm{AA} = \{4,\dots,23\}\) denote amino-acid indices. The detokenisation map returns a string by discarding indices not in \(\mathrm{AA}\). During generation and guidance, edits are restricted to positions \(i\in\{1,\dots,L-1\}\) and to candidate tokens in \(\mathrm{AA}\); CLS and EOS positions are preserved.

\section*{Conditional probability path}
Training follows the mixture discrete probability path with a convex polynomial scheduler. For a pair \((X_0,X_1)\) and \(t\in[0,1]\), the scheduler outputs
\[\alpha_t=t^n,\quad \sigma_t=1-t^n,\quad \dot\alpha_t=n\,t^{n-1},\quad \dot\sigma_t=-n\,t^{n-1},\]
with \(n>0\). The mixture path samples
\[X_t \sim p_t(\cdot\mid X_0,X_1) = \sigma_t\,\delta_{X_0} + (1-\sigma_t)\,\delta_{X_1},\]
independently per coordinate, i.e. factorised across positions. This induces single-site flips from \(X_0\) to \(X_1\) over time governed by \(\sigma_t\).

\subsection*{Teacher posterior and velocity}
Given logits for the posterior \(p_{1\mid t}(x_1\mid x_t)\) and the scheduler, the marginal velocity for the mixture path is
\[u_t = \frac{\dot\alpha_t}{1-\alpha_t}\,\big(\operatorname{softmax}(\mathrm{logits}) - \mathrm{one\_hot}(x_t)\big),\]
applied elementwise over positions and vocabulary, where \(\mathrm{one\_hot}(x_t)\) is the one-hot encoding at each position.

\section*{Student model}
A time-conditioned CNN \(f_\theta\) maps \((x_t,t)\) to logits in \(\mathbb{R}^{L\times V}\). Tokens are embedded and concatenated with a Gaussian Fourier time embedding; multiple dilated 1D convolutions with residual connections produce per-position logits. The CLS token index is preserved through training; only non-masked positions contribute to the loss.

\section*{Training objective}
The student is trained by minimizing the mixture-path generalised KL loss between the teacher posterior and the model posterior. For each position \(i\), define
\[\ell_i(x_1,x_t,t) = -\frac{\dot\alpha_t}{1-\alpha_t}\,\Big(p_{1\mid t}(x_t^i\mid x_t)-\mathbf{1}\{x_t^i=x_1^i\} + (1-\mathbf{1}\{x_t^i=x_1^i\})\,\log p_{1\mid t}(x_1^i\mid x_t)\Big),\]
with \(p_{1\mid t}=\operatorname{softmax}(f_\theta(x_t,t))\). The loss is averaged across batch and sequence elements, masking out the special mask index. Training samples \(t\sim\mathrm{Unif}[0,1-\varepsilon]\) and draws \(X_t\) from the mixture path with either uniform or masked \(X_0\), while keeping CLS intact.

\section*{Sampling without guidance}
At inference, a probability denoiser \(f_\theta\) is wrapped with the same polynomial scheduler to form a mixture-discrete Euler solver for a continuous-time Markov chain over sequences. Starting from an initial tokenised sequence \(x_{t_0}\) with random amino acids between CLS and EOS and step size \(h=1/T\), the solver iteratively computes posterior probabilities \(p_{1\mid t}\) and velocities \(u_t\), and performs Euler jumps consistent with the CTMC intensity implied by the mixture path. Only single-position edits occur per step due to the factorised update and the jump construction.

\section*{Multi-objective guided sampling}
Let \(s_j\) be differentiable or callable scoring models for objectives (e.g., antimicrobial activity, non-haemolysis, non-cytotoxicity). At time \(t\) with current tokens \(x_t\), the guidance operates as follows:
\begin{enumerate}
  \item Compute the unguided velocity \(u_t\in\mathbb{R}^{L\times V}\) from the student.
  \item For each sequence, select a single editable position \(i\in\{1,\dots,L-1\}\) that currently holds an amino-acid token.
  \item Form candidate sequences by replacing \(x_t^i\) with every candidate token in \(\mathrm{AA}\setminus\{x_t^i\}\).
  \item Evaluate improvements \(\Delta s_j\) for each candidate relative to the base \(x_t\); aggregate a ranking score and a directional score and combine into
  \[\Delta S = \mathrm{z\_score\_norm}(\text{avg-rank}) + \lambda\,\mathrm{z\_score\_norm}\Big(\sum_j w_j\,\Delta s_j\Big),\]
  with importance weights \(w_j\) and hyperparameter \(\lambda\).
  \item Optionally apply homopolymer penalties to candidates that create runs of length \(\ge 3\) at the edited position by multiplying factors \(\exp(-\gamma)\).
  \item Exponentiate the combined scores to produce multiplicative factors \(\exp(\beta\,\Delta S)\) and rescale the off-diagonal components of \(u_t\) at the selected position by these factors; enforce probability conservation by setting the self-transition component to the negative sum of the off-diagonals.
  \item Apply an Euler jump at the selected position with probability \(1-\exp(-h\,\lambda_i)\), where \(\lambda_i\) is the intensity (sum of positive off-diagonal rates) at the position after guidance. For efficiency, a calibrated variant with intensity scaling is used.
\end{enumerate}
This procedure implements single-position edits per step, maintains CLS/EOS, and biases transitions toward candidates preferred by the weighted objectives while remaining consistent with the CTMC discretisation.

\section*{Configuration and constraints}
Sequence initialisation uses CLS at position 0, EOS at position \(L-1\), and random amino acids elsewhere. The polynomial scheduler order \(n\) controls the flip schedule along time. Guidance hyperparameters include the number of simplex divisions for weight vectors, the rank/angle parameters, homopolymer penalty strength, sampling temperature, and step count \(T\). The amino-acid index range \([4,23]\) and the minimum editable position ensure edits are restricted to biologically interpretable tokens and exclude special indices.

\section*{Outputs}
Sampling yields batches of sequences \(x_1\), which are detokenised to strings and scored by the objective models. Outputs include FASTA files of sequences and CSV files with per-objective scores. Model training checkpoints store the student network parameters for reuse in sampling.
